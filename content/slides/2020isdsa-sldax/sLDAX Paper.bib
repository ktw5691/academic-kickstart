
@article{Adjerid2018,
  title = {Big Data in Psychology: {{A}} Framework for Research Advancement},
  author = {Adjerid, Idris and Kelley, Ken},
  year = {2018},
  month = oct,
  volume = {73},
  pages = {899--917},
  doi = {10.1037/amp0000190},
  abstract = {The potential for big data to provide value for psychology is significant. However, the pursuit of big data remains an uncertain and risky undertaking for the average psychological researcher. In this article, we address some of this uncertainty by discussing the potential impact of big data on the type of data available for psychological research, addressing the benefits and most significant challenges that emerge from these data, and organizing a variety of research opportunities for psychology. Our article yields two central insights. First, we highlight that big data research efforts are more readily accessible than many researchers realize, particularly with the emergence of open-source research tools, digital platforms, and instrumentation. Second, we argue that opportunities for big data research are diverse and differ both in their fit for varying research goals, as well as in the challenges they bring about. Ultimately, our outlook for researchers in psychology using and benefiting from big data is cautiously optimistic. Although not all big data efforts are suited for all researchers or all areas within psychology, big data research prospects are diverse, expanding, and promising for psychology and related disciplines.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Adjerid/2018/adjerid-2018-big_data_in_psychology.pdf},
  journal = {American Psychologist},
  language = {en},
  number = {7}
}

@article{Altszyler2017,
  title = {The Interpretation of Dream Meaning: {{Resolving}} Ambiguity Using Latent Semantic Analysis in a Small Corpus of Text},
  shorttitle = {The Interpretation of Dream Meaning},
  author = {Altszyler, Edgar and Ribeiro, Sidarta and Sigman, Mariano and Slezak, Diego Fern{\'a}ndez},
  year = {2017},
  month = sep,
  volume = {56},
  pages = {178--187},
  doi = {10.1016/j.concog.2017.09.004},
  abstract = {Computer-based dreams content analysis relies on word frequencies within predefined categories in order to identify different elements in text. As a complementary approach, we explored the capabilities and limitations of word-embedding techniques to identify word usage patterns among dream reports. These tools allow us to quantify words associations in text and to identify the meaning of target words. Word-embeddings have been extensively studied in large datasets, but only a few studies analyze semantic representations in small corpora. To fill this gap, we compared Skip-gram and Latent Semantic Analysis (LSA) capabilities to extract semantic associations from dream reports. LSA showed better performance than Skip-gram in small size corpora in two tests. Furthermore, LSA captured relevant word associations in dream collection, even in cases with low-frequency words or small numbers of dreams. Word associations in dreams reports can thus be quantified by LSA, which opens new avenues for dream interpretation and decoding.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Altszyler et al/2017/Altszyler et al. - 2017 - The interpretation of dream meaning Resolving amb.pdf},
  journal = {Consciousness and Cognition},
  language = {en},
  number = {1}
}

@article{Beck1989,
  title = {Prediction of Eventual Suicide in Psychiatric Inpatients by Clinical Ratings of Hopelessness},
  author = {Beck, Aaron T. and Brown, Gary and Steer, Robert A.},
  year = {1989},
  month = apr,
  volume = {57},
  pages = {309--310},
  doi = {10.1037/0022-006X.57.2.309},
  abstract = {A 9-point clinical rating scale was used to assess the severity of hopelessness in 141 patients hospitalized with suicidal ideation. The patients were followed from 5 to 10 years, and 10 (7.1\%) eventually committed suicide. The mean hopelessness rating for the patients committing suicide was significantly higher than that for the patients not committing suicide. A cutoff score of 6 or above successfully predicted 9 (90.0\%) of those committing suicide. The results supported previous findings in which self-reported hopelessness on the Beck Hopelessness Scale was reported to predict suicide in both psychiatric outpatients and inpatients.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Beck/1989/beck-1989-prediction_of_eventual_suicide_in_psychiatric_inpatients_by_clinical_ratings_of.pdf},
  journal = {Journal of Consulting and Clinical Psychology},
  language = {en},
  number = {2}
}

@article{Blei2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  volume = {3},
  pages = {993--1022},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Blei/2003/blei-2003-latent_dirichlet_allocation.pdf},
  journal = {Journal of Machine Learning Research}
}

@inproceedings{Blei2006a,
  title = {Correlated Topic Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Blei, David M. and Lafferty, John D.},
  editor = {Weiss, Y and Sch{\"o}lkopf, B. and Platt, J. C.},
  year = {2006},
  volume = {18},
  pages = {147--154},
  publisher = {{MIT Press}},
  address = {{Vancouver, BC}},
  abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document
arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to
also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model
(CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Blei/2006/blei-2006-correlated_topic_models.pdf}
}

@inproceedings{Blei2008,
  title = {Supervised Topic Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Blei, David M. and McAuliffe, Jon D.},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  year = {2008},
  volume = {20},
  pages = {121--128},
  publisher = {{MIT Press}},
  address = {{Vancouver, BC}},
  abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Blei/2008/blei-2008-supervised_topic_models.pdf}
}

@incollection{Blei2009,
  title = {Topic Models},
  booktitle = {Text Mining: {{Classification}}, Clustering, and Applications},
  author = {Blei, David M. and Lafferty, John D.},
  editor = {Srivastava, Ashok N. and Sahami, Mehran},
  year = {2009},
  month = jun,
  publisher = {{Chapman and Hall/CRC}},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Blei/2009/blei-2009-topic_models.pdf},
  isbn = {978-1-4200-5940-3},
  language = {en},
  series = {Data {{Mining}} and {{Knowledge Discovery}}}
}

@article{Blei2010,
  title = {Supervised Topic Models},
  author = {Blei, David M. and McAuliffe, Jon D.},
  year = {2010},
  abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a va- riety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational meth- ods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response val- ues for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amend- ments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Blei/2010/blei-2010-supervised_topic_models.pdf},
  journal = {arXiv}
}

@article{Bleidorn2019,
  title = {Using Machine Learning to Advance Personality Assessment and Theory},
  author = {Bleidorn, Wiebke and Hopwood, Christopher James},
  year = {2019},
  volume = {23},
  pages = {190--203},
  doi = {10.1177/1088868318772990},
  abstract = {Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Bleidorn/2019/bleidorn-2019-using_machine_learning_to_advance_personality_assessment_and_theory.pdf},
  journal = {Personality and Social Psychology Review},
  language = {en},
  number = {2}
}

@misc{Bouchet-Valat2019,
  title = {{{SnowballC}}: {{Snowball}} Stemmers Based on the {{C}} 'libstemmer' {{UTF}}-8 Library},
  author = {{Bouchet-Valat}, Milan},
  year = {2019},
  abstract = {An R interface to the C 'libstemmer' library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary. Currently supported languages are Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish and Turkish.}
}

@article{Box1980,
  title = {Sampling and {{Bayes}}' Inference in Scientific Modelling and Robustness},
  author = {Box, George E. P.},
  year = {1980},
  volume = {143},
  pages = {383--430},
  doi = {10.2307/2982063},
  abstract = {Scientific learning is an iterative process employing Criticism and Estimation. Correspondingly the formulated model factors into two complementary parts--a predictive part allowing model criticism, and a Bayes posterior part allowing estimation. Implications for significance tests, the theory of precise measurement and for ridge estimates are considered. Predictive checking functions for transformation, serial correlation, bad values, and their relation with Bayesian options are considered. Robustness is seen from a Bayesian viewpoint and examples are given. For the bad value problem a comparison with M estimators is made.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Box/1980/box-1980-sampling_and_bayes'_inference_in_scientific_modelling_and_robustness.pdf},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  number = {4}
}

@book{Cohen1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {Second},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Hillsdale, NJ}},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Cohen/1988/cohen-1988-statistical_power_analysis_for_the_behavioral_sciences.pdf},
  isbn = {0-8058-0283-5}
}

@incollection{Cohen1988a,
  title = {Multiple Regression and Correlation Analysis},
  booktitle = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {Second},
  pages = {407--467},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Hillsdale, NJ}},
  isbn = {0-8058-0283-5}
}

@article{Cohen1992,
  title = {A Power Primer},
  author = {Cohen, Jacob},
  year = {1992},
  month = jul,
  volume = {112},
  pages = {155--159},
  doi = {10.1037/0033-2909.112.1.155},
  abstract = {One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product\textendash moment correlation, (3) the difference between independent rs, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Cohen/1992/cohen-1992-a_power_primer.pdf},
  journal = {Psychological Bulletin},
  number = {1}
}

@article{Cox1971,
  title = {A Note on Polynomial Response Functions for Mixtures},
  author = {Cox, D. R.},
  year = {1971},
  volume = {58},
  pages = {155--159},
  doi = {10.1093/biomet/58.1.155},
  abstract = {An alternative is given to the canonical polynomials of Scheff\'e for experiments with mixtures. The object is to obtain individual parameters with an interpretation closer to that of parameters in ordinary polynomial response functions.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Cox/1971/cox-1971-a_note_on_polynomial_response_functions_for_mixtures.pdf},
  journal = {Biometrika},
  number = {1}
}

@article{Deerwester1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K},
  year = {1990},
  volume = {41},
  pages = {391--407},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Deerwester/1990/deerwester-1990-indexing_by_latent_semantic_analysis.pdf},
  journal = {Journal of the American Society for Information Science},
  language = {en},
  number = {6}
}

@book{Deng2018,
  title = {Deep Learning in Natural Language Processing},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  publisher = {{Springer Nature Singapore}},
  address = {{New York, NY}},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Deng/2018/deng-2018-deep_learning_in_natural_language_processing.pdf},
  isbn = {978-981-10-5209-5},
  language = {en}
}

@article{Dias2004,
  title = {An Empirical Comparison of {{EM}}, {{SEM}} and {{MCMC}} Performance for Problematic {{Gaussian}} Mixture Likelihoods},
  author = {Dias, Jos{\'e} G. and Wedel, Michel},
  year = {2004},
  month = oct,
  volume = {14},
  pages = {323--332},
  doi = {10.1023/B:STCO.0000039481.32211.5a},
  abstract = {We compare EM, SEM, and MCMC algorithms to estimate the parameters of the Gaussian mixture model. We focus on problems in estimation arising from the likelihood function having a sharp ridge or saddle points. We use both synthetic and empirical data with those features. The comparison includes Bayesian approaches with different prior specifications and various procedures to deal with label switching. Although the solutions provided by these stochastic algorithms are more often degenerate, we conclude that SEM and MCMC may display faster convergence and improve the ability to locate the global maximum of the likelihood function.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Dias/2004/dias-2004-an_empirical_comparison_of_em,_sem_and_mcmc_performance_for_problematic.pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {4}
}

@article{Finch2018,
  title = {The Use of Topic Modeling with Latent {{Dirichlet}} Analysis with Open-Ended Survey Items},
  author = {Finch, W. Holmes and Finch, Maria E. Hern{\'a}ndez and McIntosh, Constance E. and Braun, Claire},
  year = {2018},
  volume = {4},
  pages = {403--424},
  doi = {10.1037/tps0000173},
  abstract = {Open-ended questions are a common component in surveys and questionnaires that are
used in the social sciences. Such items can provide researchers with insights into
respondents' attitudes and opinions that cannot be easily gleaned from closed-response
items based on a traditional Likert-type response format. However, the use of openended
items can also be associated with a set of analytic problems, particularly in terms
of identifying coherent themes that are supported by the data. Topic models present the
researcher with a statistically based tool for identifying underlying topics within text,
based on how words group together, much in the way that factor analysis uses
correlations among observed variables to identify potential latent variables. The current
study demonstrates the use of topic models with open-ended response items in order to
illustrate how the resulting topics can both provide insights into respondents' attitudes
and create variables that can be used in data analyses with closed-ended item responses.
A full illustration of topic modeling in this context is given and discussion of the utility
of these models is provided.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Finch/2018/finch-2018-the_use_of_topic_modeling_with_latent_dirichlet_analysis_with_open-ended_survey.pdf},
  journal = {Translational Issues in Psychological Science},
  number = {4}
}

@article{garten2018dictionaries,
  title = {Dictionaries and Distributions: {{Combining}} Expert Knowledge and Large Scale Textual Data Content Analysis},
  author = {Garten, Justin and Hoover, Joe and Johnson, Kate M. and Boghrati, Reihane and Iskiwitch, Carol and Dehghani, Morteza},
  year = {2018},
  volume = {50},
  pages = {344--361},
  publisher = {{Springer}},
  doi = {10.3758/s13428-017-0875-9},
  abstract = {Theory-driven text analysis has made extensive use of psychological concept dictionaries, leading to a wide range of important results. These dictionaries have generally been applied through word count methods which have proven to be both simple and effective. In this paper, we introduce Distributed Dictionary Representations (DDR), a method that applies psychological dictionaries using semantic similarity rather than word counts. This allows for the measurement of the similarity between dictionaries and spans of text ranging from complete documents to individual words. We show how DDR enables dictionary authors to place greater emphasis on construct validity without sacrificing linguistic coverage. We further demonstrate the benefits of DDR on two real-world tasks and finally conduct an extensive study of the interaction between dictionary size and task performance. These studies allow us to examine how DDR and word count methods complement one another as tools for applying concept dictionaries and where each is best applied. Finally, we provide references to tools and resources to make this method both available and accessible to a broad psychological audience.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Garten/2018/garten-2018-dictionaries_and_distributions.pdf},
  journal = {Behavior Research Methods},
  number = {1}
}

@article{Gelman2019,
  title = {R-Squared for {{Bayesian}} Regression Models},
  author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
  year = {2019},
  volume = {73},
  pages = {307--309},
  doi = {10.1080/00031305.2018.1549100},
  abstract = {The usual definition of R2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the expected variance of the errors.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Gelman/2019/gelman-2019-r-squared_for_bayesian_regression_models.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {3}
}

@incollection{Geweke1992,
  title = {Evaluating the Accuracy of Sampling-Based Approaches to the Calculation of Posterior Moments},
  booktitle = {Bayesian {{Statistics}}},
  author = {Geweke, John},
  editor = {Bernardo, J. M. and Berger, J. O. and Dawid, A. P. and Smith, A. F. M.},
  year = {1992},
  edition = {Fourth},
  publisher = {{Oxford University Press}},
  address = {{Oxford}},
  abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods from spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Geweke/1992/geweke-1992-evaluating_the_accuracy_of_sampling-based_approaches_to_the_calculation_of.pdf},
  keywords = {data augmentation,Gibbs sampling,mixed estimation,Monte Carlo integration,Tobit model}
}

@article{Greer1997,
  title = {Analysis of Variance with Ipsative Measures},
  author = {Greer, Tammy and Dunlap, William P.},
  year = {1997},
  month = jun,
  volume = {2},
  pages = {200--207},
  doi = {10.1037/1082-989X.2.2.200},
  abstract = {Measures with more than 1 score per participant, when the total for each participant equals the same constant, are said to be ipsative. Ipsativity occurs when data are percentages, with each participant's total equal to 100\%, or when data are ranks, with each participant's total equal to the sum of the ranks. When ipsative measures are analyzed with analysis of variance (ANOVA), certain sums of squares equal 0, and the average intercorrelation among measures is negative. These characteristics of ipsativity may result in violations of ANOVA assumptions, producing an inflated Type I error rate and affecting power. The purpose of this Monte Carlo study was to empirically examine the extent to which ANOVA is affected by ipsative data. Findings indicated that, with few exceptions, ANOVA worked quite well with ipsative data. Not only were Type I error rates well preserved, but power was nearly equivalent to that with nonipsative data.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Greer_Dunlap/1997/Greer and Dunlap - 1997 - Analysis of variance with ipsative measures.pdf},
  journal = {Psychological Methods},
  language = {en},
  number = {2}
}

@inproceedings{Griffiths2002,
  title = {A Probabilistic Approach to Semantic Representation},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth Annual Conference}} of {{Cognitive Science Society}}},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  year = {2002},
  abstract = {Semantic networks produced from human data have statistical properties that cannot be easily captured by spatial representations. We explore a probabilistic approach to semantic representation that explicitly models the probability with which words occur in different contexts, and hence captures the probabilistic relationships between words. We show that this representation has statistical properties consistent with the large-scale structure of semantic networks constructed by humans, and trace the origins of these properties.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Griﬃths/2002/griﬃths-2002-a_probabilistic_approach_to_semantic_representation.pdf},
  language = {en}
}

@article{Griffiths2004a,
  title = {Finding Scientific Topics},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  year = {2004},
  month = apr,
  volume = {101},
  pages = {5228--5235},
  doi = {10.1073/pnas.0307752101},
  abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \&amp; Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Griffiths/2004/griffiths-2004-finding_scientific_topics.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  number = {S1}
}

@article{Grun2011,
  title = {Topicmodels: {{An R}} Package for Fitting Topic Models},
  author = {Gr{\"u}n, Bettina and Hornik, Kurt},
  year = {2011},
  month = may,
  volume = {40},
  doi = {10.18637/jss.v040.i13},
  abstract = {Topic models allow the probabilistic modeling of term frequency occurrences in documents. The fitted model can be used to estimate the similarity between documents as well as between a set of specified keywords using an additional layer of latent variables which are referred to as topics. The R package topicmodels provides basic infrastructure for fitting topic models based on data structures from the text mining package tm. The package includes interfaces to two algorithms for fitting topic models: the variational expectation-maximization algorithm provided by David M. Blei and co-authors and an algorithm using Gibbs sampling by Xuan-Hieu Phan and co-authors.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Grün/2011/grün-2011-topicmodels.pdf},
  journal = {Journal of Statistical Software},
  number = {13}
}

@inproceedings{Halpern2012,
  title = {A Comparison of Dimensionality Reduction Techniques for Unstructured Clinical Text},
  booktitle = {{{ICML}} 2012 {{Workshop}} on {{Clinical Data Analysis}}},
  author = {Halpern, Yoni and Horng, Steven and Nathanson, Larry A and Shapiro, Nathan I and Sontag, David},
  year = {2012},
  address = {{Edinburgh, Scotland, UK}},
  abstract = {Much of clinical data is free text, which is challenging to use together with machine learning, visualization tools, and clinical decision rules. In this paper, we compare supervised and unsupervised dimensionality reduction techniques, including the recently proposed sLDA and MedLDA algorithms, on clinical texts. We evaluate each dimensionality reduction method by using them as features for two important prediction problems that arise in emergency departments: predicting whether a patient has an infection, which can progress to sepsis, and predicting the likelihood of a patient being admitted to the Intensive Care Unit (used for risk stratification). We find that, on this data, existing supervised dimensionality reduction techniques perform better than unsupervised techniques only for very low dimensional representations.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Halpern/2012/halpern-2012-a_comparison_of_dimensionality_reduction_techniques_for_unstructured_clinical.pdf},
  language = {en}
}

@phdthesis{He2013,
  title = {Text Mining and {{IRT}} for Psychiatric and Psychological Assessment},
  author = {He, Qiwei},
  year = {2013},
  doi = {10.3990/1.9789036500562},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/He/2013/he-2013-text_mining_and_irt_for_psychiatric_and_psychological_assessment.pdf;/Users/kwilcox3/Zotero/storage/YPTGJPJR/He - 2013 - Text mining and IRT for psychiatric and psychologi.pdf},
  school = {University of Twente},
  type = {Dissertation}
}

@article{Hecht2019,
  title = {Integrating out Nuisance Parameters for Computationally More Efficient {{Bayesian}} Estimation \textendash{} {{An}} Illustration and Tutorial},
  author = {Hecht, Martin and Gische, Christian and Vogel, Daniel and Zitzmann, Steffen},
  year = {2019},
  month = sep,
  pages = {1--11},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2019.1647432},
  abstract = {Bayesian estimation has become very popular. However, run time of Bayesian models is often unsatisfactorily high. In this illustration, we show how to reduce run time by (a) integrating out nuisance model parameters and by (b) reformulating the model based on covariances and means. The core concept is to use the sample scatter matrix which is in our case Wishart distributed with the model-implied covariance matrix as the scale matrix. To illustrate this approach, we choose the popular multi-level null (intercept-only) model, provide a step-by-step instruction on how to implement this model in a multi-purpose Bayesian software, and show how structural equation modeling techniques can be employed to bypass mathematically challenging derivations. A simulation study showed that run time is considerably reduced and an empirical example illustrates our approach. Further, we show how the JAGS sampling progress can be monitored and stopped automatically when convergence and precision criteria are reached.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Hecht/2019/hecht-2019-integrating_out_nuisance_parameters_for_computationally_more_efficient_bayesian.pdf},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  language = {en}
}

@article{Heidelberger1983,
  title = {Simulation Run Length Control in the Presence of an Initial Transient},
  author = {Heidelberger, Philip and Welch, Peter D.},
  year = {1983},
  month = dec,
  volume = {31},
  pages = {1109--1144},
  doi = {10.1287/opre.31.6.1109},
  abstract = {This paper studies the estimation of the steady state mean of an output sequence from a discrete event simulation. It considers the problem of the automatic generation of a confidence interval of prespecified width when there is an initial transient present. It explores a procedure based on Schruben's Brownian bridge model for the detection of nonstationarity and a spectral method for estimating the variance of the sample mean. The procedure is evaluated empirically for a variety of output sequences. The performance measures considered are bias, confidence interval coverage, mean confidence interval width, mean run length, and mean amount of deleted data. If the output sequence contains a strong transient, then inclusion of a test for stationarity in the run length control procedure results in point estimates with lower bias, narrower confidence intervals, and shorter run lengths than when no check for stationarity is performed. If the output sequence contains no initial transient, then the performance measures of the procedure with a stationarity test are only slightly degraded from those of the procedure without such a test. If the run length is short relative to the extent of the initial transient, the stationarity tests may not be powerful enough to detect the transient, resulting in a procedure with unreliable point and interval estimates.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Heidelberger/1983/heidelberger-1983-simulation_run_length_control_in_the_presence_of_an_initial_transient.pdf},
  journal = {Operations Research},
  language = {en},
  number = {6}
}

@article{Hofmann2001,
  title = {Unsupervised Learning by Probabilistic Latent Semantic Analysis},
  author = {Hofmann, Thomas},
  year = {2001},
  volume = {42},
  pages = {177--196},
  doi = {10.1023/A:1007617005950},
  abstract = {This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Hofmann/2001/hofmann-2001-unsupervised_learning_by_probabilistic_latent_semantic_analysis.pdf},
  journal = {Machine Learning},
  language = {en}
}

@article{Hoogland1998,
  title = {Robustness Studies in Covariance Structure Modeling: {{An}} Overview and a Meta-Analysis},
  author = {Hoogland, Jeffrey J. and Boomsma, Anne},
  year = {1998},
  volume = {26},
  pages = {329--367},
  doi = {10.1177/0049124198026003003},
  abstract = {In covariance structure modeling, several estimation methods are available. The robustness of an estimator against specific violations of assumptions can be determined empirically by means of a Monte Carlo study. Many such studies in covariance structure analysis have been published, but the conclusions frequently seem to contradict each other. An overview of robustness studies in covariance structure analysis is given, and an attempt is made to generalize findings. Robustness studies are described and distinguished from each other systematically by means of certain characteristics. These characteristics serve as explanatory variables in a meta-analysis concerning the behavior of parameter estimators, standard error estimators, and goodness-of-fit statistics when the model is correctly specified.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Hoogland/1998/hoogland-1998-robustness_studies_in_covariance_structure_modeling.pdf},
  journal = {Sociological Methods \& Research},
  number = {3}
}

@article{Iliev2015,
  title = {Automated Text Analysis in Psychology: Methods, Applications, and Future Developments},
  shorttitle = {Automated Text Analysis in Psychology},
  author = {Iliev, Rumen and Dehghani, Morteza and Sagi, Eyal},
  year = {2015},
  month = jun,
  volume = {7},
  pages = {265--290},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2014.30},
  abstract = {Recent years have seen rapid developments in automated text analysis methods focused on measuring psychological and demographic properties. While this development has mainly been driven by computer scientists and computational linguists, such methods can be of great value for social scientists in general, and for psychologists in particular. In this paper, we review some of the most popular approaches to automated text analysis from the perspective of social scientists, and give examples of their applications in different theoretical domains. After describing some of the pros and cons of these methods, we speculate about future methodological developments, and how they might change social sciences. We conclude that, despite the fact that current methods have many disadvantages and pitfalls compared to more traditional methods of data collection, the constant increase of computational power and the wide availability of textual data will inevitably make automated text analysis a common tool for psychologists.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Iliev/2015/iliev-2015-automated_text_analysis_in_psychology.pdf},
  journal = {Language and Cognition},
  language = {en},
  number = {2}
}

@article{Jelodar2019,
  title = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling: {{Models}}, Applications, a Survey},
  author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
  year = {2019},
  month = jun,
  volume = {78},
  pages = {15169--15211},
  doi = {10.1007/s11042-018-6894-4},
  abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Jelodar/2019/jelodar-2019-latent_dirichlet_allocation_(lda)_and_topic_modeling.pdf},
  journal = {Multimedia Tools and Applications},
  language = {en},
  number = {11}
}

@inproceedings{Jones2015,
  title = {A Coefficient of Determination for Topic Models},
  booktitle = {Joint {{Statistical Meetings}}},
  author = {Jones, Thomas W.},
  year = {2015},
  month = aug,
  pages = {25},
  address = {{Seattle, Washington}},
  abstract = {This document proposes a new (old) metric for evaluating goodness of fit in topic models, the coefficient of determination, or R2. Within the context of topic modeling, R2 has the same interpretation that it does when used in a broader class of statistical models. Reporting R2 with topic models addresses two current problems in topic modeling: a lack of standard cross-contextual evaluation metrics for topic modeling and ease of communication with lay audiences. This paper proposes that R2 should be reported as a standard metric when constructing topic models.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Jones/2015/jones-2015-a_coefficient_of_determination_for_topic_models.pdf},
  language = {en}
}

@article{Kim2014,
  title = {Convolutional Neural Networks for Sentence Classification},
  author = {Kim, Yoon},
  year = {2014},
  month = sep,
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  archivePrefix = {arXiv},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Kim/2014/kim-2014-convolutional_neural_networks_for_sentence_classification.pdf},
  journal = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  language = {en}
}

@article{Kim2017,
  title = {Statistical and Qualitative Analyses of Students' Answers to a Constructed Response Test of Science Inquiry Knowledge},
  author = {Kim, Seohyun and Kwak, Minho and {Cardozo-Gaibisso}, Lourdes and Buxton, Cory and Cohen, Allan S.},
  year = {2017},
  volume = {1},
  pages = {82--102},
  abstract = {Objective: We report on a comparative study of the language used by middle school students in their answers to a constructed response test of science inquiry knowledge. Background: Text analyses using statistical models have been conducted across a number of disciplines to identify topics in a journal, to extract topics in Twitter messages, and to investigate political preferences. In education, relatively few studies have analyzed the text of students' written answers to investigate topics underlying the answers. Methodology: Two types of linguistic analysis were compared to investigate their utility in understanding students' learning of scientific investigation practices. A statistical method, latent Dirichlet allocation (LDA), was used to extract topics from the texts of student responses. In the LDA model, topics are viewed as multinomial distributions over the vocabulary of documents. These topics were examined for content and used to characterize student responses on the constructed response items. The change from pre-test to post-test in proportions of use of each of the topics was related to students' learning. Next, a qualitative method, systemic functional linguistic (SFL) analysis, was used to analyze the text of student responses on the same test of science inquiry knowledge. Student assessments were analyzed for two linguistic features that are important for convincing scientific communication: technical vocabulary usage and high lexical density. In this way, we investigated whether human judgement regarding the changes observed from texts based on the SFL framework agreed with the inference regarding the changes observed from the texts through LDA. Research questions: Two research questions were investigated in this study: (1) What do the LDA and SFL analyses tell us about students' answers? (2) What are the similarities and differences of the two analyses? Data: The data for this study were taken from an NSF-funded host study on teaching science inquiry skills to middle school students who were a mix of both native English speakers and English-language learners. The primary objective was to enable participants to learn to take ownership of scientific language through the use of language-rich science investigation practices. The LDA analysis used a sample of 252 students' pre-and post-assessments. The SFL analysis used a second sample of 90 students' pre- and post-assessments. Results: In the LDA analysis, three topics were detected in student responses: ``preponderance of everyday language (Topic 1),'' ``preponderance of general academic language (Topic 2),'' and ``preponderance of discipline-specific language (Topic 3).'' Students' use of topics changed from pre-test to post-test. Students on the post-test tended to have higher proportions of Topic 3 than students on the pre-test. In the SFL analysis, students tended to use more technical vocabulary and have higher lexical density in their written responses on the post-test than on the pre-test. Discussion: Results from the LDA and SFL analyses suggest that students responded using more discipline-specific language on the post-test than on the pre-test. In addition, the results of the two linguistic features from the SFL analysis, technical vocabulary usage and lexical density, were compared with the results from the LDA analysis. \textbullet{} Conclusion: Results of the LDA and SFL analyses were consistent with each other and clearly showed that students improved in their ability to use the discipline-specific and academic terminology of the language of scientific communication.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Kim/2017/kim-2017-statistical_and_qualitative_analyses_of_students'_answers_to_a_constructed.pdf},
  journal = {Journal of Writing Analytics},
  keywords = {constructed response items,latent Dirichlet allocation,systemic functional linguistic analysis,text analysis,topic models,writing analytics},
  number = {1}
}

@inproceedings{Kim2017a,
  title = {A Mixture Partial Credit Model Analysis Using Language-Based Covariates},
  booktitle = {Quantitative {{Psychology}}},
  author = {Kim, Seohyun and Kwak, Minho and Cohen, Allan S.},
  editor = {{van der Ark}, L. Andries and Wiberg, Marie and Culpepper, Steven A. and Douglas, Jeffrey A. and Wang, Wen-Chung},
  year = {2017},
  volume = {196},
  pages = {321--333},
  publisher = {{Springer International Publishing}},
  address = {{Asheville, NC}},
  doi = {10.1007/978-3-319-56294-0_28},
  abstract = {A mixture partial credit model (MixPCM) can be used to classify examinees into discrete latent classes based on their performance on items scored in multiple ordered categories. Characterizing the latent classes, however, is not always straightforward, particularly when analyzing text from constructed responses. This is because there may be information in the constructed responses that is not captured by the scores. Latent Dirichlet allocation (LDA) is a statistical model that has been used to detect latent topics in textual data. The topics can be used to characterize documents, such as answers on a constructed-response test, as mixtures of the topics. In this study, we used one of the topics from the LDA as a covariate in a MixPCM to help characterize the different latent classes detected by the MixPCM.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Kim/2017/kim-2017-a_mixture_partial_credit_model_analysis_using_language-based_covariates.pdf},
  language = {en},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}}
}

@article{Kjell2019,
  title = {Semantic Measures: {{Using}} Natural Language Processing to Measure, Differentiate, and Describe Psychological Constructs},
  author = {Kjell, Oscar N E and Kjell, Katarina and Garcia, Danilo and Sikstr{\"o}m, Sverker},
  year = {2019},
  volume = {24},
  pages = {92--115},
  doi = {10.1037/met0000191},
  abstract = {Psychological constructs, such as emotions, thoughts, and attitudes are often measured by asking individuals to reply to questions using closed-ended numerical rating scales. However, when asking people about their state of mind in a natural context (``How are you?''), we receive open-ended answers using words (``Fine and happy!'') and not closed-ended answers using numbers (``7'') or categories (``A lot''). Nevertheless, to date it has been difficult to objectively quantify responses to open-ended questions. We develop an approach using open-ended questions in which the responses are analyzed using natural language processing (Latent Semantic Analyses). This approach of using open-ended, semantic questions is compared with traditional rating scales in nine studies (N ϭ 92\textendash{} 854), including two different study paradigms. The first paradigm requires participants to describe psychological aspects of external stimuli (facial expressions) and the second paradigm involves asking participants to report their subjective well-being and mental health problems. The results demonstrate that the approach using semantic questions yields good statistical properties with competitive, or higher, validity and reliability compared with corresponding numerical rating scales. As these semantic measures are based on natural language and measure, differentiate, and describe psychological constructs, they have the potential of complementing and extending traditional rating scales.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Kjell/2019/kjell-2019-semantic_measures.pdf},
  journal = {Psychological Methods},
  language = {en},
  number = {1}
}

@article{Kovacs2019,
  title = {Language-Style Similarity and Social Networks},
  author = {Kovacs, Balazs and Kleinbaum, Adam M.},
  year = {2019},
  month = dec,
  doi = {10.1177/0956797619894557},
  abstract = {This research demonstrates that linguistic similarity predicts network-tie formation and that friends exhibit linguistic convergence over time. In Study 1, we analyzed the linguistic styles and the emerging social network of a complete cohort of 285 students. In Study 2, we analyzed a large-scale data set of online reviews. In both studies, we collected data in two waves to examine changes in both social networks and linguistic styles. Using the Linguistic Inquiry and Word Count (LIWC) framework, we analyzed the text of students' essays and of 1.7 million reviews by 159,651 Yelp reviewers. Consistent with our theory, results showed that similarity in linguistic style corresponded to a higher likelihood of friendship formation and persistence and that friendship ties, in turn, corresponded to a convergence in linguistic style. We discuss the implications of the coevolution of linguistic styles and social networks, which contribute to the formation of relational echo chambers.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Kovacs_Kleinbaum/2019/Kovacs and Kleinbaum - 2019 - Language-style similarity and social networks.pdf},
  journal = {Psychological Science},
  language = {en}
}

@article{Liou2014,
  title = {Autoencoder for Words},
  author = {Liou, Cheng-Yuan and Cheng, Wei-Chen and Liou, Jiun-Wei and Liou, Daw-Ran},
  year = {2014},
  month = sep,
  volume = {139},
  pages = {84--96},
  doi = {10.1016/j.neucom.2013.09.055},
  abstract = {This paper presents a training method that encodes each word into a different vector in semantic space and its relation to low entropy coding. Elman network is employed in the method to process word sequences from literary works. The trained codes possess reduced entropy and are used in ranking, indexing, and categorizing literary works. A modification of the method to train the multi-vector for each polysemous word is also presented where each vector represents a different meaning of its word. These multiple vectors can accommodate several different meanings of their word. This method is applied to the stylish analyses of two Chinese novels, Dream of the Red Chamber and Romance of the Three Kingdoms.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Liou/2014/liou-2014-autoencoder_for_words.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Liu1994,
  title = {The Collapsed {{Gibbs}} Sampler in {{Bayesian}} Computations with Applications to a Gene Regulation Problem},
  author = {Liu, Jun S.},
  year = {1994},
  month = sep,
  volume = {89},
  pages = {958--966},
  doi = {10.1080/01621459.1994.10476829},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Liu/1994/liu-1994-the_collapsed_gibbs_sampler_in_bayesian_computations_with_applications_to_a.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {427}
}

@article{Lovibond1995,
  title = {The Structure of Negative Emotional States: {{Comparison}} of the {{Depression Anxiety Stress Scales}} ({{DASS}}) with the {{Beck Depression}} and {{Anxiety Inventories}}},
  shorttitle = {The Structure of Negative Emotional States},
  author = {Lovibond, P. F. and Lovibond, S. H.},
  year = {1995},
  month = mar,
  volume = {33},
  pages = {335--343},
  doi = {10.1016/0005-7967(94)00075-U},
  abstract = {The psychometric properties of the Depression Anxiety Stress Scales (DASS) were evaluated in a normal sample of N = 717 who were also administered the Beck Depression Inventory (BDI) and the Beck Anxiety Inventory (BAI). The DASS was shown to possess satisfactory psychometric properties, and the factor structure was substantiated both by exploratory and confirmatory factor analysis. In comparison to the BDI and BAI, the DASS scales showed greater separation in factor loadings. The DASS Anxiety scale correlated 0.81 with the BAI, and the DASS Depression scale correlated 0.74 with the BDI. Factor analyses suggested that the BDI differs from the DASS Depression scale primarily in that the BDI includes items such as weight loss, insomnia, somatic preoccupation and irritability, which fail to discriminate between depression and other affective states. The factor structure of the combined BDI and BAI items was virtually identical to that reported by Beck for a sample of diagnosed depressed and anxious patients, supporting the view that these clinical states are more severe expressions of the same states that may be discerned in normals. Implications of the results for the conceptualisation of depression, anxiety and tension/stress are considered, and the utility of the DASS scales in discriminating between these constructs is discussed.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Lovibond_Lovibond/1995/Lovibond and Lovibond - 1995 - The structure of negative emotional states Compar.pdf},
  journal = {Behaviour Research and Therapy},
  language = {en},
  number = {3}
}

@book{Lynch2007,
  title = {Introduction to Applied {{Bayesian}} Statistics and Estimation for Social Scientists},
  author = {Lynch, Scott M.},
  year = {2007},
  publisher = {{Springer Science+Business Media}},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Lynch/2007/lynch-2007-introduction_to_applied_bayesian_statistics_and_estimation_for_social_scientists.pdf},
  isbn = {978-0-387-71264-2}
}

@article{Magnusson2019,
  title = {{{DOLDA}}: {{A}} Regularized Supervised Topic Model for High-Dimensional Multi-Class Regression},
  author = {Magnusson, M\aa ns and Jonsson, Leif and Villani, Mattias},
  year = {2019},
  month = jun,
  doi = {10.1007/s00180-019-00891-1},
  abstract = {Generating user interpretable multi-class predictions in data-rich environments with many classes and explanatory covariates is a daunting task. We introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), a supervised topic model for multiclass classification that can handle many classes as well as many covariates. To handle many classes we use the recently proposed Diagonal Orthant probit model (Johndrow et al., in: Proceedings of the sixteenth international conference on artificial intelligence and statistics, 2013) together with an efficient Horseshoe prior for variable selection/shrinkage (Carvalho et al. in Biometrika 97:465\textendash 480, 2010). We propose a computationally efficient parallel Gibbs sampler for the new model. An important advantage of DOLDA is that learned topics are directly connected to individual classes without the need for a reference class. We evaluate the model's predictive accuracy and scalability, and demonstrate DOLDA's advantage in interpreting the generated predictions.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Magnusson/2019/magnusson-2019-dolda.pdf},
  journal = {Computational Statistics},
  language = {en}
}

@article{Marquardt1974,
  title = {Test Statistics for Mixture Models},
  author = {Marquardt, Donald W. and Snee, Ronald D.},
  year = {1974},
  month = nov,
  volume = {16},
  pages = {533--537},
  doi = {10.2307/1267604},
  abstract = {Regression models of the forms proposed by Scheff\'e and by Becker have been widely and usefully applied to describe the response surfaces of mixture systems. These models do not contain a constant term. It has been common practice to test the statistical significance of these mixture models by the same statistical procedures used for other regression models whose constant term is absent (e. g., because the regression must pass through the origin). In this paper we show that the common practice produces misleading results for mixtures. The mixture models require a different set of F,R2, and RA2 statistics. The correct mixture statistics correspond to a physically consistent null hypothesis and are also consistent with the expression of the mixture model in the older "slack-variable" form. An illustrative example is included.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Marquardt/1974/marquardt-1974-test_statistics_for_mixture_models.pdf},
  journal = {Technometrics},
  language = {en},
  number = {4}
}

@article{Mikolov2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Mikolov et al/2013/Mikolov et al. - 2013 - Efficient estimation of word representations in ve.pdf},
  journal = {arXiv},
  keywords = {Computer Science - Computation and Language},
  language = {en}
}

@article{Muthen2002,
  title = {How to Use a {{Monte Carlo}} Study to Decide on Sample Size and Determine Power},
  author = {Muth{\'e}n, Linda K. and Muth{\'e}n, Bengt O.},
  year = {2002},
  volume = {9},
  pages = {599--620},
  doi = {10.1207/S15328007SEM0904},
  abstract = {A common question asked by researchers is, ``What sample size do I need for my study?'' Over the years, several rules of thumb have been proposed. In reality there is no rule of thumb that applies to all situations. The sample size needed for a study de- pends on many factors, including the size of the model, distribution of the variables, amount of missing data, reliability of the variables, and strength of the relations among the variables. The purpose of this article is to demonstrate howsubstantive re- searchers can use a Monte Carlo study to decide on sample size and determine power. Two models are used as examples, a confirmatory factor analysis (CFA) model and a growth model. The analyses are carried out using the Mplus program (Muth\'en \& Muth\'en, 1998)},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Muthén/2002/muthén-2002-how_to_use_a_monte_carlo_study_to_decide_on_sample_size_and_determine_power.pdf},
  journal = {Structural Equation Modeling},
  number = {4}
}

@article{obeid2019automated,
  title = {Automated Detection of Altered Mental Status in Emergency Department Clinical Notes: {{A}} Deep Learning Approach},
  author = {Obeid, Jihad S. and Weeda, Erin R. and Matuskowitz, Andrew J. and Gagnon, Kevin and Crawford, Tami and Carr, Christine M. and Frey, Lewis J.},
  year = {2019},
  volume = {19},
  pages = {164},
  publisher = {{Springer}},
  doi = {10.1186/s12911-019-0894-9},
  abstract = {Background. Machine learning has been used extensively in clinical text classification tasks. Deep learning approaches using word embeddings have been recently gaining momentum in biomedical applications. In an effort to automate the identification of altered mental status (AMS) in emergency department provider notes for the purpose of decision support, we compare the performance of classic bag-of-words-based machine learning classifiers and novel deep learning approaches. Methods. We used a case-control study design to extract an adequate number of clinical notes with AMS and non-AMS based on ICD codes. The notes were parsed to extract the history of present illness, which was used as the clinical text for the classifiers. The notes were manually labeled by clinicians. As a baseline for comparison, we tested several traditional bag-of-words based classifiers. We then tested several deep learning models using a convolutional neural network architecture with three different types of word embeddings, a pre-trained word2vec model and two models without pre-training but with different word embedding dimensions. Results. We evaluated the models on 1130 labeled notes from the emergency department. The deep learning models had the best overall performance with an area under the ROC curve of 98.5\% and an accuracy of 94.5\%. Pre-training word embeddings on the unlabeled corpus reduced training iterations and had performance that was statistically no different than the other deep learning models. Conclusion. This supervised deep learning approach performs exceedingly well for the detection of AMS symptoms in clinical text in our environment. Further work is needed for the generalizability of these findings, including evaluation of these models in other types of clinical notes and other environments. The results seem promising for the ultimate use of these types of classifiers in combination with other information derived from the electronic health records as input for clinical decision support.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Obeid/2019/obeid-2019-automated_detection_of_altered_mental_status_in_emergency_department_clinical.pdf},
  journal = {BMC Medical Informatics and Decision Making},
  number = {1}
}

@article{Ophir2020,
  title = {The {{Turker}} Blues: {{Hidden}} Factors behind Increased Depression Rates among {{Amazon}}'s {{Mechanical Turkers}}},
  shorttitle = {The {{Turker Blues}}},
  author = {Ophir, Yaakov and Sisso, Itay and Asterhan, Christa S. C. and Tikochinski, Refael and Reichart, Roi},
  year = {2020},
  month = jan,
  volume = {8},
  pages = {65--83},
  doi = {10.1177/2167702619865973},
  abstract = {Data collection from online platforms, such as Amazon's Mechanical Turk (MTurk), has become popular in clinical research. However, there are also concerns about the representativeness and the quality of these data for clinical studies. The present work explores these issues in the specific case of major depression. Analyses of two large data sets gathered from MTurk (Sample 1: N = 2,692; Sample 2: N = 2,354) revealed two major findings: First, failing to screen for inattentive and fake respondents inflates the rates of major depression artificially and significantly (by 18.5\%\textendash 27.5\%). Second, after cleaning the data sets, depression in MTurk is still 1.6 to 3.6 times higher than general population estimates. Approximately half of this difference can be attributed to differences in the composition of MTurk samples and the general population (i.e., sociodemographics, health, and physical activity lifestyle). Several explanations for the other half are proposed, and practical data-quality tools are provided.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Ophir et al/2020/Ophir et al. - 2020 - The Turker blues Hidden factors behind increased .pdf},
  journal = {Clinical Psychological Science},
  language = {en},
  number = {1}
}

@article{Packard2020,
  title = {Thinking of You: {{How}} Second-Person Pronouns Shape Cultural Success},
  shorttitle = {Thinking of {{You}}},
  author = {Packard, Grant and Berger, Jonah},
  year = {2020},
  month = feb,
  doi = {10.1177/0956797620902380},
  abstract = {Why do some cultural items succeed and others fail? Some scholars have argued that one function of the narrative arts is to facilitate feelings of social connection. If this is true, cultural items that activate personal connections should be more successful. The present research tested this possibility in the context of second-person pronouns. We argue that rather than directly addressing the audience, communicating norms, or encouraging perspective taking, second-person pronouns can encourage audiences to think of someone in their own lives. Textual analysis of songs ranked in the Billboard charts (N = 4,200), as well as controlled experiments (total N = 2,921), support this possibility, demonstrating that cultural items that use more second-person pronouns are liked and purchased more. These findings demonstrate a novel way in which second-person pronouns make meaning, how pronouns' situated use (object case vs. subject case) may shape this meaning, and how psychological factors shape the success of narrative arts.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Packard_Berger/2020/Packard and Berger - 2020 - Thinking of you How second-person pronouns shape .pdf},
  journal = {Psychological Science},
  language = {en}
}

@article{Papastamoulis2016,
  title = {Label.Switching: {{An R}} Package for Dealing with the Label Switching Problem in {{MCMC}} Outputs},
  author = {Papastamoulis, Panagiotis},
  year = {2016},
  month = feb,
  volume = {69},
  doi = {10.18637/jss.v069.c01},
  abstract = {Label switching is a well-known and fundamental problem in Bayesian estimation of mixture or hidden Markov models. In case that the prior distribution of the model parameters is the same for all states, then both the likelihood and posterior distribution are invariant to permutations of the parameters. This property makes Markov chain Monte Carlo (MCMC) samples simulated from the posterior distribution non-identifiable. In this paper, the label.switching package is introduced. It contains one probabilistic and seven deterministic relabeling algorithms in order to post-process a given MCMC sample, provided by the user. Each method returns a set of permutations that can be used to reorder the MCMC output. Then, any parametric function of interest can be inferred using the reordered MCMC sample. A set of user-defined permutations is also accepted, allowing the researcher to benchmark new relabeling methods against the available ones.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Papastamoulis/2016/papastamoulis-2016-label.pdf},
  journal = {Journal of Statistical Software},
  number = {1}
}

@article{Pennebaker2003,
  title = {Psychological Aspects of Natural Language Use: {{Our}} Words, Our Selves},
  shorttitle = {Psychological {{Aspects}} of {{Natural Language Use}}},
  author = {Pennebaker, James W. and Mehl, Matthias R. and Niederhoffer, Kate G.},
  year = {2003},
  month = feb,
  volume = {54},
  pages = {547--577},
  doi = {10.1146/annurev.psych.54.101601.145041},
  abstract = {The words people use in their daily lives can reveal important aspects of their social and psychological worlds. With advances in computer technology, text analysis allows researchers to reliably and quickly assess features of what people say as well as subtleties in their linguistic styles. Following a brief review of several text analysis programs, we summarize some of the evidence that links natural word use to personality, social and situational fluctuations, and psychological interventions. Of particular interest are findings that point to the psychological value of studying particles\textemdash parts of speech that include pronouns, articles, prepositions, conjunctives, and auxiliary verbs. Particles, which serve as the glue that holds nouns and regular verbs together, can serve as markers of emotional state, social identity, and cognitive styles.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Pennebaker/2003/pennebaker-2003-psychological_aspects_of_natural_language_use.pdf},
  journal = {Annual Review of Psychology},
  language = {en},
  number = {1}
}

@techreport{Pennebaker2015,
  title = {The Development and Psychometric Properties of {{LIWC2015}}},
  author = {Pennebaker, James W. and Boyd, Ryan L. and Jordan, Kayla and Blackburn, Kate},
  year = {2015},
  address = {{Austin, TX}},
  institution = {{University of Texas at Austin}},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Pennebaker/2015/pennebaker-2015-the_development_and_psychometric_properties_of_liwc2015.pdf}
}

@inproceedings{Perotte2011,
  title = {Hierarchically Supervised Latent {{Dirichlet}} Allocation},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Perotte, Adler J. and Wood, Frank and Elhadad, Noemie and Bartlett, Nicholas},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2609--2617},
  abstract = {We introduce hierarchically supervised latent Dirichlet allocation (HSLDA), a
model for hierarchically and multiply labeled bag-of-word data. Examples of such
data include web pages and their placement in directories, product descriptions
and associated categories from product hierarchies, and free-text clinical records
and their assigned diagnosis codes. Out-of-sample label prediction is the primary
goal of this work, but improved lower-dimensional representations of the bagof-
word data are also of interest. We demonstrate HSLDA on large-scale data
from clinical document labeling and retail product categorization tasks. We show
that leveraging the structure from hierarchical labels improves out-of-sample label
prediction substantially when compared to models that do not.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Perotte/2011/perotte-2011-hierarchically_supervised_latent_dirichlet_allocation.pdf}
}

@article{Popping2015,
  title = {Analyzing Open-Ended Questions by Means of Text Analysis Procedures},
  author = {Popping, Roel},
  year = {2015},
  volume = {128},
  pages = {23--39},
  doi = {10.1177/0759106315597389},
  abstract = {Assume one has open-ended questions in a survey and seriously wants to analyze the answers to these questions. This text discusses a number of choices when a thematic text analysis is to be applied. It starts with requirements to be posed of the open-ended questions themselves and sketches choices in the development of a code system. Coding can be performed from an instrumental or a representational perspective. First, the coding is performed from the investigator's point of view, and can be performed by a computer program. Secondly, the point of view of the respondent is acknowledged. Here, the computer can be used as a management tool, but the coding must be performed by a human coder. The choice of one of these methods depends on what the investigator is looking for and has consequences for how to proceed. When the representational coding is applied, there are questions of intercoder reliability. The codes to be used should be decided upon before or during the coding process.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Popping/2015/popping-2015-analyzing_open-ended_questions_by_means_of_text_analysis_procedures.pdf},
  journal = {Bulletin of Sociological Methodology/Bulletin de M\'ethodologie Sociologique},
  number = {1}
}

@inproceedings{Rabinovich2014,
  title = {The Inverse Regression Topic Model},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Rabinovich, Maxim and Blei, David M},
  year = {2014},
  volume = {32},
  address = {{Beijing, China}},
  abstract = {Taddy (2013) proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Rabinovich/undefined/rabinovich-the_inverse_regression_topic_model.pdf},
  language = {en}
}

@article{Rafaeli2019,
  title = {Digital Traces: {{New}} Data, Resources, and Tools for Psychological-Science Research},
  author = {Rafaeli, Anat and Ashtar, Shelly and Altman, Daniel},
  year = {2019},
  month = dec,
  volume = {28},
  pages = {560--566},
  doi = {10.1177/0963721419861410},
  abstract = {New technologies create and archive digital traces\textemdash records of people's behavior\textemdash that can supplement and enrich psychological research. Digital traces offer psychological-science researchers novel, large-scale data (which reflect people's actual behaviors), rapidly collected and analyzed by new tools. We promote the integration of digital-traces data into psychological science, suggesting that it can enrich and overcome limitations of current research. In this article, we review helpful data sources, tools, and resources and discuss challenges associated with using digital traces in psychological research. Our review positions digital-traces research as complementary to traditional psychological-research methods and as offering the potential to enrich insights on human psychology.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Rafaeli/2019/rafaeli-2019-digital_traces.pdf},
  journal = {Current Directions in Psychological Science},
  language = {en},
  number = {6}
}

@article{Ringer2018,
  title = {Initial Validation of Brief Measures of Suicide Risk Factors: {{Common}} Data Elements Used by the {{Military Suicide Research Consortium}}.},
  shorttitle = {Initial Validation of Brief Measures of Suicide Risk Factors},
  author = {Ringer, Fallon B. and Soberay, Kelly A. and Rogers, Megan L. and Hagan, Christopher R. and Chu, Carol and Schneider, Matt and Podlogar, Matthew C. and Witte, Tracy and {Holm-Denoma}, Jill and Plant, E. Ashby and Gutierrez, Peter M. and Joiner, Thomas E.},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {767--778},
  doi = {10.1037/pas0000519},
  abstract = {The Military Suicide Research Consortium (MSRC) developed a 57-item questionnaire assessing suicide risk factors, referred to as the Common Data Elements (CDEs), in order to facilitate data sharing and improve collaboration across independent studies. All studies funded by MSRC are required to include the CDEs in their assessment protocol. The CDEs include shortened measures of the following: current and past suicide risk, lethality and intent of past suicide attempts, hopelessness, thwarted belongingness, anxiety sensitivity, posttraumatic stress disorder symptoms, traumatic brain injury, insomnia, and alcohol abuse. This study aimed to evaluate the psychometric properties of the CDE items drawn from empirically validated measures. Exploratory factor analysis was used to examine the overall structure of the CDE items, and confirmatory factor analyses were used to evaluate the distinct properties of each scale. Internal consistencies of the CDE scales and correlations with full measures were also examined. Merged data from 3,140 participants (81.0\% military service members, 75.6\% male) across 19 MSRC-funded studies were used in analyses. Results indicated that all measures exhibited adequate internal consistency, and all CDE shortened measures were significantly correlated with the corresponding full measures with moderate to strong effect sizes. Factor analyses indicated that the shortened CDE measures performed well in comparison with the full measures. Overall, our findings suggest that the CDEs are not only brief but also provide psychometrically valid scores when assessing suicide risk and related factors that may be used in future research.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Ringer/2018/ringer-2018-initial_validation_of_brief_measures_of_suicide_risk_factors.pdf},
  journal = {Psychological Assessment},
  language = {en},
  number = {6}
}

@article{Roberts1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
  year = {1997},
  volume = {7},
  pages = {110--120},
  doi = {10.1214/aoap/1034625254},
  abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ?. When the proposal variance is appropriately scaled accord- ing to n, the sequence of stochastic processes formed by the first compo- nent of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward effi- ciency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Roberts/1997/roberts-1997-weak_convergence_and_optimal_scaling_of_random_walk_metropolis_algorithms.pdf},
  journal = {Annals of Applied Probability},
  keywords = {Markov chain Monte Carlo,Metropolis algorithm,Optimal scaling,Weak convergence},
  number = {1}
}

@article{roberts2014structural,
  title = {Structural Topic Models for Open-Ended Survey Responses},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and {Leder-Luis}, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  year = {2014},
  month = oct,
  volume = {58},
  pages = {1064--1082},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/ajps.12103},
  abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Roberts/2014/roberts-2014-structural_topic_models_for_open-ended_survey_responses.pdf},
  journal = {American Journal of Political Science},
  number = {4}
}

@article{Roberts2016,
  title = {A Model of Text for Experimentation in the Social Sciences},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
  year = {2016},
  month = jul,
  volume = {111},
  pages = {988--1003},
  doi = {10.1080/01621459.2016.1141684},
  abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of documentlevel covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Roberts/2016/roberts-2016-a_model_of_text_for_experimentation_in_the_social_sciences.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {515}
}

@article{Rohrer2017,
  title = {"{{What}} Else Are You Worried about?" {{Integrating}} Textual Responses into Quantitative Social Science Research},
  author = {Rohrer, Julia M. and Br{\"u}mmer, Martin and Schmukle, Stefan C. and Goebel, Jan and Wagner, Gert G.},
  year = {2017},
  volume = {12},
  pages = {e0182156},
  doi = {10.1371/journal.pone.0182156},
  abstract = {Open-ended questions have routinely been included in large-scale survey and panel studies, yet there is some perplexity about how to actually incorporate the answers to such questions into quantitative social science research. Tools developed recently in the domain of natural language processing offer a wide range of options for the automated analysis of such textual data, but their implementation has lagged behind. In this study, we demonstrate straightforward procedures that can be applied to process and analyze textual data for the purposes of quantitative social science research. Using more than 35,000 textual answers to the question "What else are you worried about?" from participants of the German Socio-economic Panel Study (SOEP), we (1) analyzed characteristics of respondents that determined whether they answered the open-ended question, (2) used the textual data to detect relevant topics that were reported by the respondents, and (3) linked the features of the respondents to the worries they reported in their textual data. The potential uses as well as the limitations of the automated analysis of textual data are discussed.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Rohrer/2017/rohrer-2017-what_else_are_you_worried_about.pdf},
  journal = {PLoS ONE},
  number = {7}
}

@article{Rubin1984,
  title = {Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician},
  author = {Rubin, Donald B.},
  year = {1984},
  volume = {12},
  pages = {1151--1172},
  abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Rubin/1984/rubin-1984-bayesianly_justifiable_and_relevant_frequency_calculations_for_the_applied.pdf},
  journal = {The Annals of Statistics},
  number = {4}
}

@book{Salton1983,
  title = {Introduction to Modern Information Retrieval},
  author = {Salton, Gerard and McGill, Michael J.},
  year = {1983},
  publisher = {{McGraw-Hill}},
  address = {{New York, NY}}
}

@article{schwartz2013personality,
  title = {Personality, Gender, and Age in the Language of Social Media: {{The}} Open-Vocabulary Approach},
  author = {Schwartz, H. Andrew and Eichstaedt, Johannes C. and Kern, Margaret L. and Dziurzynski, Lukasz and Ramones, Stephanie M. and Agrawal, Megha and Shah, Achal and Kosinski, Michal and Stillwell, David and Seligman, Martin E. P. and Ungar, Lyle H.},
  year = {2013},
  volume = {8},
  pages = {e73791},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pone.0073791},
  abstract = {We analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age. In our open-vocabulary technique, the data itself drives a comprehensive exploration of language that distinguishes people, finding connections that are not captured with traditional closed-vocabulary word-category analyses. Our analyses shed new light on psychosocial processes yielding results that are face valid (e.g., subjects living in high elevations talk about the mountains), tie in with other research (e.g., neurotic people disproportionately use the phrase 'sick of' and the word 'depressed'), suggest new hypotheses (e.g., an active life implies emotional stability), and give detailed insights (males use the possessive 'my' when mentioning their 'wife' or 'girlfriend' more often than females use 'my' with 'husband' or 'boyfriend'). To date, this represents the largest study, by an order of magnitude, of language and personality.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Schwartz/2013/schwartz-2013-personality,_gender,_and_age_in_the_language_of_social_media.pdf},
  journal = {PloS ONE},
  number = {9}
}

@article{Silge2016,
  title = {Tidytext: {{Text}} Mining and Analysis Using Tidy Data Principles in {{R}}},
  author = {Silge, Julia and Robinson, David},
  year = {2016},
  month = jul,
  volume = {1},
  doi = {10.21105/joss.00037},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Silge_Robinson/2016/Silge and Robinson - 2016 - tidytext Text mining and analysis using tidy data.pdf},
  journal = {Journal of Statistical Software},
  number = {3}
}

@article{Stephens2000,
  title = {Dealing with Label Switching in Mixture Models},
  author = {Stephens, Matthew},
  year = {2000},
  volume = {62},
  pages = {795--809},
  abstract = {In a Bayesian analysis of finite mixture models, parameter estimation and clustering are sometimes less straightforward than might be expected. In particular,the common practice of estimating parameters by their posterior mean, and summarizing joint posterior distributions by marginal distributions, often leads to nonsensical answers. This is due to the so-called 'label switching' problem, which is caused by symmetry in the likelihood of the model parameters. A frequent response to this problem is to remove the symmetry by using artificial identifiability constraints. We demonstrate that this fails in general to solve the problem, and we describe an alternative class of approaches, relabelling algorithms, which arise from attempting to minimize the posterior expected loss under a class of loss functions. We describe in detail one particularly simple and general relabelling algorithm and illustrate its success in dealing with the label switching problem on two examples.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Stephens/2000/stephens-2000-dealing_with_label_switching_in_mixture_models.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  language = {en},
  number = {4}
}

@book{Stone1966,
  title = {The {{General Inquirer}}: {{A}} Computer Approach to Content Analysis},
  author = {Stone, Philip J. and Dunphy, Dexter C. and Smith, Marshall S and Ogilvie, Daniel M.},
  year = {1966},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}}
}

@article{Tausczik2010a,
  title = {The Psychological Meaning of Words: {{LIWC}} and Computerized Text Analysis Methods},
  author = {Tausczik, Yla R. and Pennebaker, James W.},
  year = {2010},
  month = mar,
  volume = {29},
  pages = {24--54},
  doi = {10.1177/0261927X09351676},
  abstract = {We are in the midst of a technological revolution whereby,for the first time,researchers can link daily word use to a broad array of real-world behaviors.This article reviews several computerized text analysis methods and describes how Linguistic Inquiry and Word Count (LIWC) was created and validated. LIWC is a transparent text analysis program that counts words in psychologically meaningful categories. Empirical results using LIWC demonstrate its ability to detect meaning in a wide variety of experimental settings, including to show attentional focus, emotionality, social relationships, thinking styles, and individual differences.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Tausczik/2010/tausczik-2010-the_psychological_meaning_of_words.pdf},
  journal = {Journal of Language and Social Psychology},
  language = {en},
  number = {1}
}

@article{Teh2006,
  title = {Hierarchical {{Dirichlet}} Processes},
  author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
  year = {2006},
  month = dec,
  volume = {101},
  pages = {1566--1581},
  doi = {10.1198/016214506000000302},
  abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise." We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Teh/2006/teh-2006-hierarchical_dirichlet_processes.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {476}
}

@article{Tueller2011,
  title = {Addressing the Problem of Switched Class Labels in Latent Variable Mixture Model Simulation Studies},
  author = {Tueller, Stephen J. and Drotar, Scott and Lubke, Gitta H.},
  year = {2011},
  month = jan,
  volume = {18},
  pages = {110--131},
  doi = {10.1080/10705511.2011.534695},
  abstract = {The discrimination between alternative models and the detection of latent classes in the context of latent variable mixture modeling depends on sample size, class separation, and other aspects that are related to power. Prior to a mixture analysis it is useful to investigate model performance in a simulation study that reflects the research settings. Multiple data sets are generated under 1 or more models, and alternative models are fitted to the data. The aggregation of results over multiple data sets is complicated by the fact that mixture models are only identified up to a permutation of the class labels. Estimated class labels are arbitrary, with the effect that the estimated parameters for Class 1 could be incorrectly labeled as Class 2, Class 3, and so forth, relative to their data generating labels. In a simulation study, the detection of switched labels needs to be automated. Switched class labels are not necessarily simple to detect. This article describes different possible scenarios of switched class labels, and develops an algorithm implemented in R that (a) detects switched labels, and (b) provides information that can be used to either correct class labels or to discard a particular data set from a simulation if class labels are ambivalent. The algorithm is useful in Monte Carlo simulations involving latent variable mixture models.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Tueller/2011/tueller-2011-addressing_the_problem_of_switched_class_labels_in_latent_variable_mixture.pdf},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  language = {en},
  number = {1}
}

@book{VandenBoogaart2013,
  title = {Analyzing Compositional Data with {{R}}},
  author = {{van den Boogaart}, K. Gerald and {Tolosana-Delgado}, Raimon},
  year = {2013},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36809-7},
  abstract = {This book presents the statistical analysis of compositional data sets, i.e., data in percentages, proportions, concentrations, etc. The subject is covered from its grounding principles to the practical use in descriptive exploratory analysis, robust linear models and advanced multivariate statistical methods, including zeros and missing values, and paying special attention to data visualization and model display issues. Many illustrated examples and code chunks guide the reader into their modeling and interpretation. And, though the book primarily serves as a reference guide for the R package ``compositions,'' it is also a general introductory text on Compositional Data Analysis.
Awareness of their special characteristics spread in the Geosciences in the early sixties, but a strategy for properly dealing with them was not available until the works of Aitchison in the eighties. Since then, research has expanded our understanding of their theoretical principles and the potentials and limitations of their interpretation. This is the first comprehensive textbook addressing these issues, as well as their practical implications with regard to software.
The book is intended for scientists interested in statistically analyzing their compositional data. The subject enjoys relatively broad awareness in the geosciences and environmental sciences, but the spectrum of recent applications also covers areas like medicine, official statistics, and economics.
Readers should be familiar with basic univariate and multivariate statistics. Knowledge of R is recommended but not required, as the book is self-contained.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/van den Boogaart_Tolosana-Delgado/2013/van den Boogaart and Tolosana-Delgado - 2013 - Analyzing compositional data with R.pdf},
  isbn = {978-3-642-36809-7},
  language = {en}
}

@article{VanderLinden2007,
  title = {A Hierarchical Framework for Modeling Speed and Accuracy on Test Items},
  author = {{van der Linden}, Wim J.},
  year = {2007},
  month = sep,
  volume = {72},
  pages = {287--308},
  doi = {10.1007/s11336-006-1478-z},
  abstract = {Current modeling of response times on test items has been strongly influenced by the paradigm of experimental reaction-time research in psychology. For instance, some of the models have a parameter structure that was chosen to represent a speed-accuracy tradeoff, while others equate speed directly with response time. Also, several response-time models seem to be unclear as to the level of parametrization they represent. A hierarchical framework for modeling speed and accuracy on test items is presented as an alternative to these models. The framework allows a ``plug-and-play approach'' with alternative choices of models for the response and response-time distributions as well as the distributions of their parameters. Bayesian treatment of the framework with Markov chain Monte Carlo (MCMC) computation facilitates the approach. Use of the framework is illustrated for the choice of a normal-ogive response model, a lognormal model for the response times, and multivariate normal models for their parameters with Gibbs sampling from the joint posterior distribution.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/van der Linden/2007/van der Linden - 2007 - A hierarchical framework for modeling speed and ac.pdf},
  journal = {Psychometrika},
  number = {3}
}

@article{VanOrden2012,
  title = {Thwarted Belongingness and Perceived Burdensomeness: {{Construct}} Validity and Psychometric Properties of the {{Interpersonal Needs Questionnaire}}},
  shorttitle = {Thwarted Belongingness and Perceived Burdensomeness},
  author = {Van Orden, Kimberly A. and Cukrowicz, Kelly C. and Witte, Tracy K. and Joiner, Thomas E.},
  year = {2012},
  month = mar,
  volume = {24},
  pages = {197--215},
  doi = {10.1037/a0025358},
  abstract = {The present study examined the psychometric properties and construct validity of scores derived from the Interpersonal Needs Questionnaire (INQ) using latent variable modeling with 5 independent samples varying in age and level of psychopathology. The INQ was derived from the interpersonal theory of suicide and was developed to measure thwarted belongingness and perceived burdensomeness\textemdash{} both proximal causes of desire for suicide. Results support that thwarted belongingness and perceived burdensomeness are distinct but related constructs and that they can be reliably measured. Further, multiple-group analyses were consistent with invariance for young versus older adults and nonclinical versus clinical populations, thereby supporting the relevance of these constructs to diverse populations. Finally, both constructs demonstrated convergent associations with related interpersonal constructs\textemdash including loneliness and social support for belongingness and social worth and death ideation for burdensomeness\textemdash as well as prospective associations with suicidal ideation.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Van Orden/2012/van_orden-2012-thwarted_belongingness_and_perceived_burdensomeness.pdf},
  journal = {Psychological Assessment},
  language = {en},
  number = {1}
}

@article{Vehtari2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2017},
  volume = {27},
  pages = {1413--1432},
  doi = {10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Vehtari/2017/vehtari-2017-practical_bayesian_model_evaluation_using_leave-one-out_cross-validation_and.pdf},
  journal = {Statistics and Computing},
  keywords = {Bayesian computation,K-fold cross-validation,leave-one-out cross-validation (LOO),Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,widely applicable information criterion (WAIC),Widely applicable information criterion (WAIC)},
  number = {5}
}

@article{Watanabe2010,
  title = {Asymptotic Equivalence of {{Bayes}} Cross Validation and {{Widely Applicable Information Criterion}} in Singular Learning Theory},
  author = {Watanabe, Sumio},
  year = {2010},
  volume = {11},
  pages = {3571--3594},
  abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2{$\lambda$}/n, where {$\lambda$} is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Watanabe/2010/watanabe-2010-asymptotic_equivalence_of_bayes_cross_validation_and_widely_applicable.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Wilcox2020a,
  title = {A Text Mining Approach to Characterizing Interpersonal Stress among Nonsuicidal Self-Injury},
  author = {Wilcox, Kenneth Tyler and Jacobucci, Ross and McCloskey, Michael S. and Ammerman, Brooke A.},
  year = {2020},
  journal = {Manuscript in preparation}
}

@misc{Wilcox2020b,
  title = {Psychtm: {{Text}} Mining for Psychological Research},
  author = {Wilcox, Kenneth Tyler},
  year = {2020},
  month = feb,
  abstract = {Supports Bayesian estimation of supervised (sLDAX) and unsupervised (LDA) topic models.},
  copyright = {All rights reserved}
}

@article{Zhu2013,
  title = {Improved {{Bayesian}} Logistic Supervised Topic Models with Data Augmentation},
  author = {Zhu, Jun and Zheng, Xun and Zhang, Bo},
  year = {2013},
  month = oct,
  abstract = {Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.},
  archivePrefix = {arXiv},
  eprint = {1310.2408},
  eprinttype = {arxiv},
  file = {/Users/kwilcox3/Dropbox/zotero-lib/Zhu/2013/zhu-2013-improved_bayesian_logistic_supervised_topic_models_with_data_augmentation.pdf},
  journal = {arXiv:1310.2408 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}


